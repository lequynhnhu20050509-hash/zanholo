import feedparser
import urllib.parse
import requests
from bs4 import BeautifulSoup
from zlapi.models import Message
from config import ADMIN

des = {
    'version': "2.0.1",
    'credits': "Latte",
    'description': "T√¨m ki·∫øm b√°o Google News",
    'power': "Th√†nh vi√™n"
}

# L∆∞u t·∫°m k·∫øt qu·∫£ t√¨m ki·∫øm ƒë·ªÉ l·ªánh xem d√πng
search_cache = {}

def search_google_news(query, max_results=5):
    query_enc = urllib.parse.quote(query)
    rss_url = f"https://news.google.com/rss/search?q={query_enc}&hl=vi&gl=VN&ceid=VN:vi"
    feed = feedparser.parse(rss_url)
    results = []
    for entry in feed.entries[:max_results]:
        results.append((entry.title, entry.link))
    return results

def fetch_full_article(link):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(link, headers=headers, timeout=6)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        paragraphs = soup.find_all('p')
        content = "\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
        return content if content else "‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung chi ti·∫øt."
    except Exception as e:
        return f"‚ö†Ô∏è L·ªói khi l·∫•y b√†i b√°o: {str(e)}"

# L·ªánh t√¨m ki·∫øm b√°o
def handle_bao_command(message, message_object, thread_id, thread_type, author_id, client):
    parts = message.split(" ", 1)
    if len(parts) < 2 or not parts[1].strip():
        client.replyMessage(Message(text="‚ö†Ô∏è Vui l√≤ng nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm b√°o!"), message_object, thread_id, thread_type)
        return
    query = parts[1].strip()
    client.replyMessage(Message(text=f"üîé B·∫°n v·ª´a t√¨m ki·∫øm b√°o h√¥m nay: '{query}'"), message_object, thread_id, thread_type)

    articles = search_google_news(query, max_results=5)
    if not articles:
        client.replyMessage(Message(text=f"‚ùå Kh√¥ng t√¨m th·∫•y tin t·ª©c n√†o cho '{query}'"), message_object, thread_id, thread_type)
        return

    search_cache[author_id] = articles  # L∆∞u t·∫°m ƒë·ªÉ l·ªánh xem d√πng

    msg_lines = []
    for i, (title, link) in enumerate(articles):
        msg_lines.append(f"{i+1}. {title}\nüîó {link}")
    msg_text = "üì∞ K·∫øt qu·∫£ t√¨m ki·∫øm:\n\n" + "\n\n".join(msg_lines)
    client.replyMessage(Message(text=msg_text), message_object, thread_id, thread_type)

# L·ªánh xem full b√°o
def handle_xem_command(message, message_object, thread_id, thread_type, author_id, client):
    parts = message.split(" ", 1)
    if len(parts) < 2 or not parts[1].strip():
        client.replyMessage(Message(text="‚ö†Ô∏è Vui l√≤ng nh·∫≠p s·ªë b√†i b√°o mu·ªën xem!"), message_object, thread_id, thread_type)
        return
    if author_id not in search_cache:
        client.replyMessage(Message(text="‚ö†Ô∏è Ch∆∞a c√≥ t√¨m ki·∫øm n√†o. H√£y d√πng l·ªánh 'bao <t·ª´ kh√≥a>' tr∆∞·ªõc."), message_object, thread_id, thread_type)
        return

    try:
        index = int(parts[1].strip()) - 1
    except ValueError:
        client.replyMessage(Message(text="‚ö†Ô∏è S·ªë b√†i b√°o kh√¥ng h·ª£p l·ªá."), message_object, thread_id, thread_type)
        return

    articles = search_cache[author_id]
    if index < 0 or index >= len(articles):
        client.replyMessage(Message(text="‚ö†Ô∏è S·ªë b√†i b√°o ngo√†i ph·∫°m vi."), message_object, thread_id, thread_type)
        return

    title, link = articles[index]
    client.replyMessage(Message(text=f"üì∞ ƒêang ƒë·ªçc b√†i b√°o: {title}"), message_object, thread_id, thread_type)

    full_text = fetch_full_article(link)
    max_len = 3000
    for i in range(0, len(full_text), max_len):
        part = full_text[i:i+max_len]
        client.replyMessage(Message(text=f"{part}\n\nüîó Link g·ªëc: {link}"), message_object, thread_id, thread_type)

def TQD():
    return {
        'b√°o': handle_bao_command,
        'xem': handle_xem_command
    }